[
["index.html", "LexOPS Walkthrough Overview Useful Links", " LexOPS Walkthrough Jack Taylor 2020-02-12 Overview LexOPS is an R package and shiny app for generating word stimuli, for use in Psychology experiments. LexOPS can generate stimuli for a factorial design specified by the user, controlling for selected lexical variables. The package has an inbuilt database of features for English words (LexOPS::lexops), but the user can also use their own list of features, for English words and/or for words in other languages. This site explains most features of LexOPS, and provides some example applications. Useful Links Preprint about LexOPS: https://psyarxiv.com/7sudw Cite LexOPS: FAQ - How should I cite LexOPS GitHub Repository: https://github.com/JackEdTaylor/LexOPS "],
["introduction.html", "1 Introduction 1.1 What is LexOPS? 1.2 Installation 1.3 The Shiny App 1.4 The LexOPS Dataset", " 1 Introduction 1.1 What is LexOPS? LexOPS is an R package and shiny app for generating word stimuli that can be used in Psychology experiments. The package is designed to be as intuitive as possible, and is similar in style to the tidyverse. The shiny app is designed to be especially intuitive, and requires minimal knowledge of R to use. Here are 3 main advantages of using LexOPS to generate your word stimuli: Speed: It’s just much (much much) faster than creating well-controlled stimuli manually. Reproducibility: if a random seed is set, LexOPS pipelines will generate the same stimuli each time the code is run. This means you can openly share the code that generated your stimuli, and anyone can reproduce your result. Replicability: if no random seed is set, LexOPS pipelines will generate different stimuli each time the code is run. This means if you share your code, anyone can generate a novel set of stimuli for the same experimental design, that can be used to try and replicate any experimental result. The package began as a series of R scripts, written to automate tasks I found myself repeating again and again when designing experiments. I decided to make it into a fully-fledged package because despite the huge range of freely available lists of word norms and features, there were no task-general solutions for using such lists to generate word stimuli. 1.2 Installation Installing R You can find the latest version of R here: https://cloud.r-project.org/. Installing RStudio RStudio is a free integrated development environment for working with R. Get the latest version of RStudio here: https://www.rstudio.com/products/rstudio/. Installing LexOPS The latest version of LexOPS can be installed in R with the following code: if (!require(&quot;devtools&quot;)) install.packages(&quot;devtools&quot;) devtools::install_github(&quot;JackEdTaylor/LexOPS@*release&quot;) 1.3 The Shiny App LexOPS features an intuitive graphical user interface, in the form of a shiny app (for more on shiny apps, see https://shiny.rstudio.com/). This features useful visualisations of data and selected options, and may be more friendly to users less familiar with R. Once LexOPS is installed, the shiny app can be run with: LexOPS::run_shiny() Figure 1.1: The Generate tab of the LexOPS Shiny App A demo version of the shiny app is also available as a web app online, at https://jackt.shinyapps.io/lexops/, but it is much faster and more reliable to run it locally with the run_shiny() function. 1.4 The LexOPS Dataset LexOPS works can work with any list of features from any language (see this vignette for an example using other datasets, or this section for an example using non-English words). Even so, LexOPS has inbuilt dataset with some useful features for English. This can be called with: LexOPS::lexops For details on the variables included, see https://rdrr.io/github/JackEdTaylor/LexOPS/man/lexops.html. Note that in addition to citing LexOPS, the sources for any materials you use must also be explicitly cited, regardless of whether they are included in the built-in dataset. "],
["the-generate-pipeline.html", "2 The Generate Pipeline 2.1 Generating Stimuli 2.2 Converting to Long Format 2.3 Plotting the Design 2.4 Checking Representativeness 2.5 Generating as Many as Possible 2.6 Plotting Iterations 2.7 Custom Dataframes", " 2 The Generate Pipeline 2.1 Generating Stimuli One of the most noteworthy features of LexOPS is that it can generate controlled stimuli for any possible factorial design. This can be done in a pipeline using 3 main functions: split_by() to specify “splits” (independent variables) in the experimental design control_for() to specify variables that should be controlled for between conditions generate() to run the algorithm and generate the stimuli, using specified splits and controls A Practical Example Imagine we want to generate stimuli for a 2x2 factorial design in a Lexical Decision Task, examining whether a possible effect of bigram probability interacts with concreteness. For this, we might decide we want a stimulus set with the following features: Filtered such that at least 90% of people know each word, according to Brysbaert, Mandera, McCormick, and Keuleers (2019). Two levels of Concreteness (abstract and concrete words) according to Brysbaert, Warriner and Kuperman (2014). Two levels of Bigram Probability (low and high probability) based on SUBTLEX-UK (van Heuven, Mandera, Keuleers, &amp; Brysbaert, 2014). Controlled for word length (number of characters) exactly. Controlled for word frequency within ±0.2 Zipf, according to according to SUBTLEX-UK (van Heuven, Mandera, Keuleers, &amp; Brysbaert, 2014). 25 words for each of the generated conditions (100 stimuli in total). We can use the following R code to generate a stimulus list like this with LexOPS: library(LexOPS) stim &lt;- lexops %&gt;% subset(PK.Brysbaert &gt;= 0.9) %&gt;% split_by(CNC.Brysbaert, 1:2 ~ 4:5) %&gt;% split_by(BG.SUBTLEX_UK, 0:0.003 ~ 0.009:0.013) %&gt;% control_for(Length, 0:0) %&gt;% control_for(Zipf.SUBTLEX_UK, -0.2:0.2) %&gt;% generate(n = 25) Important notes on LexOPS (non-standard) syntax: As in the tidyverse, variables in the dataframe can be referenced outside of quotation marks. The : character is used in split_by() to specify numeric boundaries (e.g. 0.009:0.013 means any number from 0.009 to 0.013 is acceptable for this level of the variable), and in control_for() to specify tolerances (e.g. -0.2:0.2 means controls will be acceptable if within ±0.2 of the match null). The ~ character is used in split_by() to specify different levels of an independent variable (e.g. 1.5:2.5 ~ 3.5:4.5 ~ 5.5:6.5 would specify three levels). Output Let’s have a closer look at the first 5 rows of stim, which contains the output from above: item_nr A1_B1 A1_B2 A2_B1 A2_B2 match_null 1 abide merit caddy basin A2_B1 2 unspecific sinisterly typescript storefront A1_B2 3 broadly reliant giraffe coroner A1_B2 4 deftly pathos smooch sandal A2_B2 5 ultra revel pluck minty A2_B2 We can see that we have 4 conditions: A1_B1 (abstract, low probability) A1_B2 (abstract, high probability) A2_B1 (concrete, low probability) A2_B2 (concrete, high probability) Each row of the dataframe is controlled for in terms of frequency and length. The match_null variable tells us which condition stimuli were matched relative to. For instance, we can see that in row 3, items are matched relative to the word “relent”. This means that all words for item_nr 3 are within ±0.2 Zipf of the Zipf value associated with the word “relent”. By default, LexOPS will select the match_null for each item pseudo-randomly, such that each condition will be used as a match null an equal number of times (match_null = \"balanced\"), or as close to this ideal as is possible (e.g. the number of items requested may not be divisible by the number of conditions). See this FAQ section for more information on match nulls. 2.2 Converting to Long Format The table above shows the generated stimuli in wide format. This is a useful way to quickly view the stimuli and get a sense for what has been generated, but we often want to check our stimuli in more detail. The long_format() function is a quick way to convert stimuli generated in LexOPS into long format: stim_long &lt;- long_format(stim) Now we have the same stimuli in long format, with their associated values. Here are the same first 5 matched items from earlier, but in long format: item_nr condition match_null string Zipf.SUBTLEX_UK Length BG.SUBTLEX_UK CNC.Brysbaert 1 A1_B1 A2_B1 abide 3.453132 5 0.0029111 1.68 1 A1_B2 A2_B1 merit 3.577120 5 0.0122070 1.66 1 A2_B1 A2_B1 caddy 3.575405 5 0.0024618 4.32 1 A2_B2 A2_B1 basin 3.482487 5 0.0100465 4.63 2 A1_B1 A1_B2 unspecific 1.540834 10 0.0029410 1.41 2 A1_B2 A1_B2 sinisterly 1.473887 10 0.0098809 1.75 2 A2_B1 A1_B2 typescript 1.297796 10 0.0027336 4.37 2 A2_B2 A1_B2 storefront 1.473887 10 0.0096897 4.00 3 A1_B1 A1_B2 broadly 3.648528 7 0.0028142 1.68 3 A1_B2 A1_B2 reliant 3.602609 7 0.0103099 1.83 3 A2_B1 A1_B2 giraffe 3.777083 7 0.0021165 4.73 3 A2_B2 A1_B2 coroner 3.726335 7 0.0107999 4.34 4 A1_B1 A2_B2 deftly 2.329204 6 0.0025051 1.96 4 A1_B2 A2_B2 pathos 2.620015 6 0.0121742 1.48 4 A2_B1 A2_B2 smooch 2.263937 6 0.0028086 4.21 4 A2_B2 A2_B2 sandal 2.443924 6 0.0091593 4.68 5 A1_B1 A2_B2 ultra 3.572531 5 0.0029273 1.55 5 A1_B2 A2_B2 revel 3.254444 5 0.0093859 1.69 5 A2_B1 A2_B2 pluck 3.377881 5 0.0019315 4.00 5 A2_B2 A2_B2 minty 3.421647 5 0.0099473 4.11 Here we can see that indeed, the different conditions are within the boundaries we set, and that the variables used as controls are matched for items with the same item_nr. 2.3 Plotting the Design While having data in long format is undoubtably useful, it’s not very efficient if you want to check what your stimuli look like in a quick glance (especially if your stimuli number in the thousands). What you probably want to do is create a plot to see how your stimuli differ between conditions and across matched items. Thankfully, LexOPS has a handy function to do exactly that: plot_design(stim) Figure 2.1: The results of the plot_design() function for the generated stimuli. The distributions of all the numeric variables used as independent variables or controls are plotted for each condition (in a grey violin plot). The points depict the values of individual words, and points of the same colour (joined by lines) are matched items. As we’d expect, our example stimuli show the expected differences in Bigram Probability and Concreteness, while Frequency is matched closely, and Length is matched exactly. 2.4 Checking Representativeness We can also visualise how representative our stimuli are. This shows the distributions of our generated stimuli on variables, relative to possible but unused candidates. plot_sample(stim) Figure 2.2: The results of the plot_sample() function for the generated stimuli. 2.5 Generating as Many as Possible Let’s imagine that we’re not entirely sure how many stimuli we could generate using our design. It may be that the n = 25 we used earlier is considerably fewer than the number of stimuli we could generate with no problems. One way to test this is to have LexOPS generate as many stimuli as possible. We can do this by setting n = \"all\": possible_stim &lt;- LexOPS::lexops %&gt;% subset(PK.Brysbaert &gt;= 0.9) %&gt;% split_by(CNC.Brysbaert, 1:2 ~ 4:5) %&gt;% split_by(BG.SUBTLEX_UK, 0:0.003 ~ 0.009:0.013) %&gt;% control_for(Length, 0:0) %&gt;% control_for(Zipf.SUBTLEX_UK, -0.2:0.2) %&gt;% generate(n = &quot;all&quot;, match_null = &quot;random&quot;) This is much slower, as LexOPS will continue trying to generate combinations of words that fit the specified characteristics until it has exhausted all the possibilities. Nevertheless, we actually generated 101 words generated per condition with the code above. This number is likely to change slightly each time we run the pipeline, as different combinations are randomly made from all the possible combinations. That said, it is a fairly good indication of the number of possible stimuli we could generate. The 101 words we’ve managed to generate per condition here is quite a bit higher than the 25 we originally generated. Does this mean we should just request a larger stimulus list, such as n = 80, or even n = 100? Well, it depends. If we want as many stimuli as are possible, then it may make sense to just set n = \"all\", but often we only want to use as many stimuli as we need to find our effect. Also, if we use as many combinations as possible, experimenters who want to replicate our effect using a different set of stimuli will likely have fewer novel combinations available to them. Note that when n = “all”, you will get a warning if you also keep the default match null setting, match_null = “balanced”. The reason for this is explained in this FAQ section. 2.6 Plotting Iterations It is also possible to check how well LexOPS performed when generating stimuli by plotting the cumulative item generation by iterations. To do this, we can use the plot_iterations() function. As an example, let’s visualise the algorithm’s iterations when we generated as many stimuli as we could in the last section. plot_iterations(possible_stim) Figure 2.3: The cumulative number of items generated per iteration. This shows a characteristic levelling-off; iterations become increasingly less likely to successfully generate items as the pool of possible combinations is gradually exhausted. 2.7 Custom Dataframes While the LexOPS Dataset has lots of useful features for English, it isn’t exhaustive, and other languages do exist! The LexOPS functions will actually work with any dataframe, with words from any language. As an example, here’s how to generate a stimulus list of negative, neutral, and positive German words matched for length and frequency, based on the Leipzig Affective Norms for German (LANG) (Kanske &amp; Kotz, 2010). Note that the set_options() function is used to tell LexOPS that our column containing the strings is \"word\", rather than the LexOPS default of \"string\". library(readr) LANG &lt;- read_csv(&quot;kanske_kotz_2010.csv&quot;, locale=locale(encoding = &quot;latin1&quot;)) stim &lt;- LANG %&gt;% set_options(id_col = &quot;word&quot;) %&gt;% split_by(valence_mean, 1:3.5 ~ 4.75:5.25 ~ 6.5:9) %&gt;% control_for(number_of_letters, 0:0) %&gt;% control_for(frequency, -1:1) %&gt;% generate(20) Here are the first five items generated for each condition: item_nr A1 A2 A3 match_null 1 hetze blase flirt A2 2 bombe stamm komik A3 3 pein seil kuss A1 4 ärger stein humor A3 5 terror lehrer gefühl A1 For a more detailed example, try the vignette on Using Data from Custom Sources. This will show how you can use the dplyr join functions to integrate custom datasets with the LexOPS dataset. Custom datasets can also be neatly integrated into the Shiny app. See this section for a example using custom variables in the shiny app. "],
["advanced-stimulus-generation.html", "3 Advanced Stimulus Generation 3.1 Non-Stimulus Splits 3.2 Random Seeds 3.3 Map Functions as Controls 3.4 Controlling for Euclidean Distance", " 3 Advanced Stimulus Generation There are some cases when the default behaviour of the split_by(), control_for(), and generate() functions won’t quite do what you want. This section presents some extra functions and functionality for generating your stimuli. 3.1 Non-Stimulus Splits Sometimes it makes sense to use independent variables that are not related to features of the word stimuli you present. For example, you may be interested in a possible difference between contexts or tasks (such as comparing Word Naming to Lexical Decision). In this case, it makes sense to use a within-subject design, but to present different stimuli in each context. These different stimuli should still be matched across contexts. This is possible with the split_random() function. As an example, imagine we’re interested in a 2x2 interaction between the effect of words’ arousal ratings before and after a cup of coffee. We use split_random() to say that we want to create a random split in the data with two levels. Such a pipeline might look like this: stim &lt;- lexops %&gt;% split_random(2) %&gt;% split_by(AROU.Warriner, 1:3 ~ 6:8) %&gt;% control_for(Length, 0:0) %&gt;% control_for(Zipf.SUBTLEX_UK, -0.1:0.1) %&gt;% generate(50) This will create 4 conditions matched for length and frequency, where A1_B1 and A2_B1 are low arousal words, and A1_B2 and A2_B2 are high arousal words. Here are the first 5 items of each condition: item_nr A1_B1 A1_B2 A2_B1 A2_B2 match_null 1 panel snake bunch glory A1_B1 2 cobbler screech padding deathly A2_B2 3 monogrammed promiscuity linguistics exhilarated A1_B2 4 hourly unjust dismay payday A1_B1 5 rectangular heartbroken therapeutic stimulating A2_B2 These stimuli could then be used in combination with a counter-balanced design, alternating which level of B has its stimuli presented before the coffee, and which after. Note that the split_random() function will also need its own random seed (seed = ...) to replicate a specific stimulus list. 3.2 Random Seeds By default, the generate pipeline will produce novel stimulus lists each time it is run. Often you’ll want your code to be reproducible, however. To do this, we can use a random seed. Random seeds allow for replicable results from functions that produce different results each time they are run. In R, this is usually done with the set.seed() function. The set.seed() function can be used with LexOPS to write reproducible pipelines, but will yield different results between LexOPS R code and the LexOPS shiny app. To ensure that pipelines created with R code can be reproduced in the shiny app (and vice versa) it is recommended to use the seed argument of the generate() function. Setting the seed in the generate() function The following code will generate the same stimulus list each time it is run: stim &lt;- lexops %&gt;% subset(PK.Brysbaert &gt;= 0.9) %&gt;% split_by(CNC.Brysbaert, 1:2 ~ 4:5) %&gt;% split_by(BG.SUBTLEX_UK, 0:0.003 ~ 0.009:0.013) %&gt;% control_for(Length, 0:0) %&gt;% control_for(Zipf.SUBTLEX_UK, -0.2:0.2) %&gt;% generate(25, seed = 42) Setting the seed in the split_random() function If you use the split_random() function, this will also require a seed argument in order for the pipeline to be reproducible. This does not necessarily need to be the same as the seed value passed to generate(), but the shiny app will assume this is the case when translating Shiny options into R code. The following is an example of a reproducible pipeline that uses the split_random() function: my_seed &lt;- 42 stim &lt;- lexops %&gt;% split_random(2, seed = my_seed) %&gt;% split_by(AROU.Warriner, 1:3 ~ 6:8) %&gt;% control_for(Length, 0:0) %&gt;% control_for(Zipf.SUBTLEX_UK, -0.1:0.1) %&gt;% generate(50, seed = my_seed) 3.3 Map Functions as Controls Imagine you want to generate a list of stimuli, controlling for a value of similarity to the match null. This would be very difficult using the control_for() function. The problem is that the similarity values would need to be calculated n times, relative each word currently being used as a match null. The control_for_map() function tells LexOPS to recalculate the value to use as a control on each iteration of the generate() function, relative to a value associated with the string currently selected as a match null. Example applications for this include controlling for orthographic or phonological similarity. 3.3.1 Orthographic Similarity Here is an example, using control_for_map() to control for orthographic Levenshtein distance from the match null (calculated using the vwr package). This means that each string will be a distance of between 0 and 3 character insertions, deletions or replacements from the word used as the match null. library(vwr) stim &lt;- lexops %&gt;% split_by(VAL.Warriner, 1:3 ~ 4.5:5.5 ~ 7:9) %&gt;% control_for_map(levenshtein.distance, string, 0:3, name=&quot;Orth_Dist&quot;) %&gt;% generate(20) Here are the first 5 items of each condition that we generated. Note that we have 3 levels of valence, with matched items that are orthographically similar to one another: item_nr A1 A2 A3 match_null 1 hearse snare praise A1 2 fist faze food A3 3 enslave suave save A1 4 whine orgy honey A3 5 slave heavy play A2 The name argument (name = …) of the control_for_map() function will simply specify the name of the column that the calculated values should be stored as when the stimuli are in long format (from the code above, long_format(stim) will contain a column called “Orth_Dist”, containing the orthographic distances from the match null). 3.3.2 Phonological Similarity We can use a similar pipeline to match by phonological similarity. Instead of giving the string column to the function, we just have to give a column where phonemes are represented by single letters. All the generated stimuli will be within a distance of between 0 and 2 phonemic insertions, deletions, or substitutions from the word used as the match null. For fun (and to show how control_for_map() can be combined with control_for()), let’s also control for rhyme. library(vwr) stim &lt;- lexops %&gt;% split_by(VAL.Warriner, 1:3 ~ 4.5:5.5 ~ 7:9) %&gt;% control_for_map(levenshtein.distance, eSpeak.br_1letter, 0:2, name=&quot;Phon_Dist&quot;) %&gt;% control_for(Rhyme.eSpeak.br) %&gt;% generate(20) Here are the first 5 items of each condition that we generated. This time, all matched items are phonologically similar to one another: item_nr A1 A2 A3 match_null 1 bury heavy merry A2 2 jealousy lunacy legacy A3 3 lonely wily lily A3 4 killer pillar giver A1 5 saggy cabbie daddy A1 3.3.3 Other Uses The control_for_map() function can be used to control for any variable that needs to be calculated relative to the match null. The fun argument of control_for_map() should be a function that takes the data in the column contained in var as its first argument, and can take the match null’s value for var as the second argument, to return a vector of values for each entry. In addition, tol can be specified as it is in control_for(), as either a numeric tolerance, or categorical tolerance if the function outputs character vectors. The name argument should just be a character vector to name the column in the long format of the generated stimuli. If you’re wanting to use control_for_map() on your own function or data, see this vignette on controlling for semantic relatedness. The function’s documentation in the package (?control_for_map) might also be useful. Additionally, you may want to look at how the vwr functions work for comparison, as control_for_map() was originally written with these in mind. 3.4 Controlling for Euclidean Distance The control_for() function lets you give specific tolerances for individual variables. Another method of matching items, however, may be to control for Euclidean distance, weighting variables by their relative importance. The control_for_euc() function lets you do exactly this. As an example, imagine we want to split by concretness, and control for length, frequency, and age of acquisition. We might decide that length is the most important thing to match (ideally exact matching), followed by frequency and age of acquisition. This could be achieved like so: stim &lt;- lexops %&gt;% split_by(CNC.Brysbaert, 1:2 ~ 4:5) %&gt;% control_for_euc( c(Length, Zipf.BNC.Written, AoA.Kuperman), 0:1e-5, name = &quot;euclidean_distance&quot;, weights = c(1, 0.5, 0.05) ) %&gt;% generate(20) ## Generated 2/20 (10%). 171 total iterations, 0.01 success rate. ## Generated 4/20 (20%). 394 total iterations, 0.01 success rate. ## Generated 6/20 (30%). 1064 total iterations, 0.01 success rate. ## Generated 8/20 (40%). 1515 total iterations, 0.01 success rate. ## Generated 10/20 (50%). 1738 total iterations, 0.01 success rate. ## Generated 12/20 (60%). 2012 total iterations, 0.01 success rate. ## Generated 14/20 (70%). 2591 total iterations, 0.01 success rate. ## Generated 16/20 (80%). 2727 total iterations, 0.01 success rate. ## Generated 18/20 (90%). 3055 total iterations, 0.01 success rate. ## Generated 20/20 (100%). 3221 total iterations, 0.01 success rate. This method will generally be slower than standard LexOPS pipelines, but more flexible. You can also combine control_for_euc() and control_for() functions in your pipelines. For more information on controlling for Euclidean distance, and how to decide on tolerances, try the vignette on Euclidean distance. "],
["matching-individual-items.html", "4 Matching Individual Items 4.1 Example 4.2 Matching by Similarity", " 4 Matching Individual Items While the generate pipeline is usually sufficient, it’s sometimes important to tailor stimuli more precisely. For instance, it may be important that a matched word is a plausible replacement for a target word in a sentence. The match_item() function exists for this purpose. 4.1 Example Here’s an example usage of match_item(), to suggest a word matched for “elephant” in terms of: Length (exactly) Frequency (within ±0.25 Zipf) Imageability (within ±1, on a 1-7 Likert rating scale) Part of Speech (i.e. is also a noun) library(LexOPS) suggested_matches &lt;- lexops %&gt;% match_item( &quot;elephant&quot;, Length, Zipf.SUBTLEX_UK = -0.25:0.25, IMAG.Glasgow_Norms = -1:1, PoS.SUBTLEX_UK ) The suggested matches are returned in a dataframe, filtered to be within the specified tolerances, and ordered by euclidean distance from the target word (calculated using all the numeric variables used). The closest suggested match for “elephant” is “sandwich”. If we are looking for a match to fit in a sentential context, we can choose the best suitable match from this list. string euclidean_distance Length Zipf.SUBTLEX_UK IMAG.Glasgow_Norms PoS.SUBTLEX_UK sandwich 0.1277847 8 4.246820 6.7647 noun trousers 0.2015596 8 4.244371 6.6286 noun wardrobe 0.3255001 8 4.104315 6.6176 noun clothing 0.3302431 8 4.135068 6.5455 noun calendar 0.3359636 8 4.329810 6.4000 noun magazine 0.3522379 8 4.287246 6.3846 noun bungalow 0.3726770 8 4.172277 6.4242 noun envelope 0.4004126 8 4.096792 6.4706 noun festival 0.4979160 8 4.510449 6.2353 noun motorway 0.5312797 8 4.107187 6.2333 noun exercise 0.5545789 8 4.449319 6.1212 noun treasure 0.5598276 8 4.458939 6.1176 noun portrait 0.5860691 8 4.183298 6.0968 noun engineer 0.6080043 8 4.138999 6.0909 noun document 0.6314308 8 4.182166 6.0323 noun shooting 0.7013990 8 4.391130 5.9032 noun applause 0.7068317 8 4.209885 5.9143 noun darkness 0.7291092 8 4.143049 5.9118 noun 4.2 Matching by Similarity You may want to match by similarity to the target word. Thankfully this is more straightforward than in the generate pipeline (see control_for_map()). 4.2.1 Orthographic similarity Here’s an example, matching “leaflet” by orthographic similarity. We just have to calculate the similarity measure before using the match_item() function. library(LexOPS) library(vwr) library(dplyr) target_word &lt;- &quot;interesting&quot; suggested_matches &lt;- lexops %&gt;% mutate(orth_sim = as.numeric(levenshtein.distance(string, target_word))) %&gt;% match_item(target = target_word, orth_sim = 0:3) Note that some of these are misspellings or unusual words, but we could remove these by filtering (e.g. with dplyr::filter()) or matching (with match_item()) by frequency, proportion known, or familiarity ratings. string euclidean_distance orth_sim interestin 0.8288949 1 interacting 1.6577897 2 intercepting 1.6577897 2 interestingay 1.6577897 2 interestingly 1.6577897 2 interjecting 1.6577897 2 intermeshing 1.6577897 2 intersecting 1.6577897 2 uninteresting 1.6577897 2 entreating 2.4866846 3 4.2.2 Phonological Similarity To match by phonological similarity, we just have to calculate the levenshtein distance on one-letter phonemic representations, e.g. with CMU.1letter or eSpeak.br_1letter. Here we find words that are only 0 to 2 phonemic insertions, deletions, or substitutions away from “interesting”. library(LexOPS) library(vwr) library(dplyr) target_word &lt;- &quot;interesting&quot; # get the target word&#39;s pronunciation target_word_pron &lt;- lexops %&gt;% filter(string == target_word) %&gt;% pull(CMU.1letter) # find phonologically similar words suggested_matches &lt;- lexops %&gt;% mutate(phon_sim = as.numeric(levenshtein.distance(CMU.1letter, target_word_pron))) %&gt;% match_item(target_word, phon_sim = 0:2) Which gives us: string euclidean_distance phon_sim CMU.1letter entrusting 0.9515925 1 EntrAstIG encrusting 1.9031850 2 EnkrAstIG entrusted 1.9031850 2 EntrAstId instructing 1.9031850 2 InstrAktIG interest 1.9031850 2 IntrAst interested 1.9031850 2 IntrAstAd interests 1.9031850 2 IntrAsts interrupting 1.9031850 2 IntRAptIG intrastate 1.9031850 2 IntrAstet mistrusting 1.9031850 2 mIstrAstIG "],
["lexops-shiny-app.html", "5 LexOPS Shiny App 5.1 Generate 5.2 Match Item 5.3 Fetch 5.4 Visualise 5.5 Custom Variables 5.6 Random Seeds", " 5 LexOPS Shiny App The Shiny app is an interactive user interface for LexOPS, with informative visualisations and illustrations of selected options. The LexOPS Shiny app can be run locally with: LexOPS::run_shiny() The Shiny app is also available as a web app at https://jackt.shinyapps.io/lexops/, but running the app locally is recommended for reliability and speed. 5.1 Generate The Generate tab is a user interface for the “generate pipeline”. This allows you to generate stimuli for any possible factorial design. 5.1.1 Specify Design Splits (independent variables), controls, and filters can be specified in the following way: Click the ‘+’ button to add a new variable. Choose a variable from the drop-down menu. Select a source for the variable if necessary (i.e. “according to…”). Specify the boundaries or tolerances Figure 5.1: Specifying a design in the shiny app. 5.1.2 Options Here you can tell the app how many stimuli should be generated per condition, which condition should be used as the match null, and which variables to include in the long-format version of the results. Figure 1.1: Options for the Generate tab. 5.1.3 Results This is where you can see the generated stimuli. A different set of stimuli which fit the design will be generated each time the “Regenerate” button is clicked. You can switch between viewing the stimuli in wide or long format, and can download the stimuli in either format as a file: generated_stimuli.csv. Figure 5.2: Generating stimuli and viewing the results. 5.1.4 Review Once the stimuli have been generated, this section allows you to view a summary of the generated stimuli. You can view how splits, controls, and filters differ between conditions and across matched items (which calls the plot_design() function). You can also view the cumulative item generation (from the plot_iterations() function), and check the distribution of the match nulls. Figure 5.3: Reviewing generated stimuli. 5.1.5 Codify This is a handy feature that lets you translate the selected options into LexOPS R code to reproduce the design. To reproduce a specific stimulus list, set the seed in the Preferences tab. Figure 5.4: Translate selected options into LexOPS R code. 5.2 Match Item The Match Item tab is a user interface for the match_item() function. As in the Generate tab, you can specify variables that should be matched by (with tolerances relative to the target string) and filtered by (with boundaries independent of the target string). You can then view and download the suggested matches. Figure 5.5: Matching individual words in the Match tab. 5.3 Fetch The Fetch tab is an easy way to get values from the LexOPS database (or uploaded in the Custom Variables tab) for your own list of words. As an example, I may have a file, my_stimuli.csv, with the following contents: word likert_rating mean_RT pigeon 1.40 456.0 dog 5.20 423.5 wren 6.00 511.0 symbiosis 4.44 503.2 shark 4.20 482.3 I could then upload this to the Fetch tab. This will return the known values from the LexOPS dataset for my list of stimuli: Figure 5.6: Fetching the features of a stimulus list. 5.4 Visualise The Visualise tab provides many options for plotting useful information. As well as the variables included in the LexOPS dataset and custom variables, the Visualise tab can plot information from within the app, such as the generated condition. Here’s an example application of the visualise tab to look at stimuli generated in the Generate tab (in a Bigram Probability x Concreteness design). Points can be coloured by things like generated condition, and individual words can be identified by hovering over the points. Figure 2.1: Example usage of the Visualise tab. 5.5 Custom Variables The Custom Variables tab is useful for integrating variables not in the LexOPS dataset to the app. Importantly, the words which have custom variables can be in any language. These variables can then be used within the app for generating stimuli. Custom variables are only available for the current session. Here is an example in which the variables from the Leipzig Affective Norms for German (LANG) (Kanske &amp; Kotz, 2010) are added to the LexOPS shiny app. These variables can then be used in the Generate tab. Figure 2.2: Using stimuli for languages other than English. The Custom Variables tab uses dplyr’s join functions. An equivalent to using custom variables in R code would be to either join the custom variables to the LexOPS dataset using dplyr’s join functions, or to just run the generate pipeline on a dataframe of custom variables (see this section for an example). 5.6 Random Seeds The seed can be set in the Preferences tab to reproduce specific lists of generated stimuli. This means that the exact same list of words will be generated each time you click “Regenerate”. Figure 5.7: Setting the seed in the Shiny app. If you “codify” your selected options, the generated code will also set the seed as the same value, such that a stimulus list can be reproduced outside of the shiny app. Note that in order to reproduce stimulus lists generated with R code within the shiny app, the seed should be set using the seed argument of the generate() and split_random() functions. See this section on random seeds in the generate pipeline for some examples. "],
["vignettes.html", "6 Vignettes", " 6 Vignettes These vignettes have been written to demonstrate in more detail how common problems can be solved with LexOPS. Click on a vignette to find out more. 1) Higher Level Controls (control_for_map): The control_for() function works well when your variable is 1-dimensional, with a single value for each word, as it is for variables like Length, Frequency or Concreteness. Things become slightly trickier, however, when controlling for distance or similarity values, which can be calculated for each unique combination of words (i.e. \\(n^2\\) values). One easy solution is to use control_for_map() to pass a function which should be used to calculate the value between any two words. Simple examples for controlling for orthographic and phonological similarity are available in the package bookdown site. This vignette demonstrates how to build your own function for control_for_map(), which in this example controls for semantic similarity. 2) Using Data from Custom Sources: The built-in variables of LexOPS are useful but not exhaustive. Thankfully, LexOPS can work with any suitable list of features. For this example, we will join the Lancaster Sensorimotor norms to Engelthaler and Hills’ humour ratings, and the in-built LexOPS dataset (LexOPS::lexops). We can then use this to generate stimuli with a visual rating by humour interaction, controlling for length and frequency. 3) Creating Custom Levels: By default, LexOPS will split by a single variable for each use of split_by(), and will create items for each factorial cell. For instance, splitting by arousal into 2 levels, and emotional valence into 3 levels, would result in 6 factorial cells. But what if we want to generate items for just 2 of these 6 factorial cells? We can do this by creating a factor/character vector column in our data which will represent suitability for each factorial cell. This vignette provides an example, where we want to compare high arousal, negative emotional words to low arousal neutral words. 4) Euclidean Distance: Euclidean distance can be a useful way of matching by or controlling for multiple variables with greater flexibility than dealing with the variables individually. This vignette explains how weighted and unweighted Euclidean distance is calculated in LexOPS, gives example usage with euc_dists() and match_item(), and introduces the control_for_euc() function. The latter allows you to generate stimuli controlling for Euclidean distance to a match null within a Generate pipeline. "],
["faq.html", "7 FAQ", " 7 FAQ How should I cite LexOPS? If you use LexOPS to generate your stimuli, please cite it! Make sure you also cite the sources for any datasets you use, regardless of whether they are included the LexOPS in-built dataset. Here is how to cite LexOPS: Taylor, J. E., Beith, A., &amp; Sereno, S. C. (2019, September 17). LexOPS: An R Package and User Interface for the Controlled Generation of Word Stimuli. https://doi.org/10.31234/osf.io/7sudw What is a “match null”? The “match null” is a term used in the generate pipeline, specified as the match_null argument in the generate() function. Whenever you control for variables in the generate pipeline, items are always matched relative to one of the specified conditions. The condition that items are matched relative to is referred to as the match null. The default for the generate() function is to use match_null = \"balanced\". This means that each condition is used as a match null an equal number of times, or as close to this as possible, in a random order. For example, if you specify 4 conditions (A1_B1, A1_B2, A2_B1, A2_B2) and ask for 50 stimuli (n = 50), by default the algorithm will randomly select two conditions to be used 12 times as match nulls, and two conditions to be used 13 times. Since match_null = \"balanced\" works by dividing n by the number of conditions, this default will not work when n = \"all\" as the generate() function will not know how many stimuli will be generated in the stimulus list. This is why you’ll get a warning if these options are used together. One alternative may be to use match_null = \"random\". Inclusive match nulls are also possible with match_null = \"inclusive\", where stimuli will only be produced which are within the specified tolerances of each other stimulus they are matched to. It is also possible to use a specific condition as the match null for each item. If you want to use condition A1_B2 as your null for instance, you can do this by specifying match_null = \"A1_B2\". The match null used for each generated item is stored in the wide and long formats of the output from the generate pipeline, as the variable match_null. "]
]
