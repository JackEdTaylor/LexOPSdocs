[
["index.html", "LexOPS Walkthrough Overview", " LexOPS Walkthrough Jack Taylor 2019-08-03 Overview LexOPS is an R package and web/shiny app for generating word stimuli, for use in Psychology experiments. LexOPS can generate stimuli for a factorial design specified by the user, controlling for selected lexical variables. The package has an inbuilt database of features for English words (LexOPS::lexops), but the user can also use their own list of features, for English words and/or for words in other languages. This site provides an introduction to the main features of LexOPS, and some example applications. "],
["introduction.html", "1 Introduction 1.1 What is LexOPS? 1.2 Installation 1.3 The Shiny App 1.4 The LexOPS Dataset", " 1 Introduction 1.1 What is LexOPS? LexOPS is an R package and shiny app for generating word stimuli that can be used in Psychology experiments. The package is designed to be as intuitive as possible, and is similar in style to the tidyverse. The shiny app is designed to be especially intuitive, and requires minimal knowledge of R to use. Here are 3 main advantages of using LexOPS to generate your word stimuli: Speed: It’s just much (much much) faster than creating well-controlled stimuli manually. Reproducibility: if a random seed is set, LexOPS pipelines will generate the same stimuli each time the code is run. This means you can openly share the code that generated your stimuli, and anyone can reproduce your result. Replicability: if no random seed is set, LexOPS pipelines will generate different stimuli each time the code is run. This means if you share your code, anyone can generate a novel set of stimuli for the same experimental design, that can be used to try and replicate any experimental result. The package began as a series of R scripts, written to automate tasks I found myself repeating again and again when designing experiments. I decided to make it into a fully-fledged package because despite the huge range of freely available lists of word norms and features, there were no task-general solutions for using such lists to generate word stimuli. 1.2 Installation The latest version of LexOPS can be installed with the following R code: if (!require(&quot;devtools&quot;)) install.packages(&quot;devtools&quot;) devtools::install_github(&quot;JackEdTaylor/LexOPS&quot;) Install R here: https://cloud.r-project.org/ Install RStudio (reccommended IDE) here: https://www.rstudio.com/products/rstudio/ 1.3 The Shiny App LexOPS features an intuitive shiny app (for more on shiny apps, see https://shiny.rstudio.com/). This features useful visualisations of data, and may be more friendly to users less familiar with R. Once LexOPS is installed, the shiny app can be run with: LexOPS::run_shiny() Figure 1.1: The Generate tab of the LexOPS Shiny App The shiny app is also available as a web app online, at https://jackt.shinyapps.io/lexops/, but it is usually more reliable to run it locally with the run_shiny() function. 1.4 The LexOPS Dataset LexOPS has a built-in dataset of English word features, which can be called with: LexOPS::lexops For details on the variables included, see https://rdrr.io/github/JackEdTaylor/LexOPS/man/lexops.html. "],
["the-generate-pipeline.html", "2 The Generate Pipeline 2.1 Generating Stimuli 2.2 Converting to Long Format 2.3 Plotting the Design 2.4 Generating as Many as Possible 2.5 Plotting Iterations", " 2 The Generate Pipeline 2.1 Generating Stimuli One of the most noteworthy features of LexOPS is that it can generate controlled stimuli for any possible factorial design. This can be done in a pipeline using 3 main functions: split_by() to specify “splits” (independent variables) in the experimental design control_for() to specify variables that should be controlled for between conditions generate() to run the algorithm and generate the stimuli As an example, we may want to generate stimuli for a 2x2 factorial design in a Lexical Decision Task, examining whether a possible effect of bigram probability interacts with concreteness. Let’s imagine that we want abstract and concrete words (according to Brysbaert, Warriner and Kuperman (2014)), of high and low bigram probability (based on SUBTLEX-UK (van Heuven, Mandera, Keuleers, &amp; Brysbaert, 2014)). We also want to filter the data, such that our stimuli only consist of words that at least 90% of people actually know (according to Brysbaert, Mandera, and Keuleers (2018)). Finally, we want to control for the potential confounds of word length exactly and word frequency (according to SUBTLEX-UK (van Heuven, Mandera, Keuleers, &amp; Brysbaert, 2014)) within ±0.2 Zipf, and would like 25 words per condition. library(LexOPS) stim &lt;- LexOPS::lexops %&gt;% subset(PK.Brysbaert &gt;= 0.9) %&gt;% split_by(CNC.Brysbaert, 1:2 ~ 4:5) %&gt;% split_by(BG.SUBTLEX_UK, 0:0.003 ~ 0.009:0.013) %&gt;% control_for(Length, 0:0) %&gt;% control_for(Zipf.SUBTLEX_UK, -0.2:0.2) %&gt;% generate(n = 25) Important notes on LexOPS (non-standard) syntax: As in the tidyverse, variables in the dataframe can be referenced outside of quotation marks. The : character is used in split_by() to specify numeric boundaries (e.g. 0.009:0.013 means any number from 0.009 to 0.013 is acceptable for this level of the variable), and in and control_for() to specify tolerances (e.g. -0.2:0.2 means controls will be acceptable if within ±0.2 of the match null). The ~ character is used in split_by() to specify different levels of an independent variable (e.g. 1.5:2.5 ~ 3.5:4.5 ~ 5.5:6.5 would specify three levels of an independent variable). Let’s have a look at the first 5 rows of stim as an example: item_nr A1_B1 A1_B2 A2_B1 A2_B2 match_null 1 abide merit caddy basin A2_B1 2 acceptably beseeching typescript fourteenth A1_B2 3 snobby relent duffel shelve A1_B2 4 subtlety riveting bagpipes headless A2_B2 5 adequacy endeared typeface whiteout A2_B2 We can see that we have 4 conditions: A1_B1 (abstract, low probability) A1_B2 (abstract, high probability) A2_B1 (concrete, low probability) A2_B2 (concrete, high probability) Each row of the table is controlled for in terms of frequency and length. The match_null variable tells us which condition stimuli were matched relative to. For instance, we can see that in row 3, items are matched relative to the word “snobby”. This means (for example) that all words for item_nr 3 are within ±0.2 Zipf of the Zipf value associated with the word snobby. By default, LexOPS will select the match_null for each item pseudo-randomly, such that each condition will be used as a match null an equal number of times, or as close to this ideal as is possible (e.g. the number of items requested may not be divisible by the number of conditions). 2.2 Converting to Long Format The table above shows the generated stimuli in wide format. This is a useful way to quickly view the stimuli and get a sense for what has been generated, but we often want to check our stimuli in more detail. The long_format() function is a quick way to convert stimuli generated in LexOPS into long format: stim_long &lt;- long_format(stim) Now we have the same stimuli in long format, with their associated values. Here are the same first 5 matched items from earlier, but in long format: item_nr condition match_null string Zipf.SUBTLEX_UK Length BG.SUBTLEX_UK CNC.Brysbaert 1 A1_B1 A2_B1 abide 3.453132 5 0.0029111 1.68 1 A1_B2 A2_B1 merit 3.577120 5 0.0122070 1.66 1 A2_B1 A2_B1 caddy 3.575405 5 0.0024618 4.32 1 A2_B2 A2_B1 basin 3.482487 5 0.0100465 4.63 2 A1_B1 A1_B2 acceptably 1.540834 10 0.0026093 1.48 2 A1_B2 A1_B2 beseeching 1.473887 10 0.0095482 1.88 2 A2_B1 A1_B2 typescript 1.297796 10 0.0027336 4.37 2 A2_B2 A1_B2 fourteenth 1.540834 10 0.0120087 4.21 3 A1_B1 A1_B2 snobby 2.564967 6 0.0017474 1.81 3 A1_B2 A1_B2 relent 2.376977 6 0.0108419 1.69 3 A2_B1 A1_B2 duffel 2.318985 6 0.0020055 4.12 3 A2_B2 A1_B2 shelve 2.459164 6 0.0105992 4.41 4 A1_B1 A2_B2 subtlety 2.913220 8 0.0025943 1.54 4 A1_B2 A2_B2 riveting 2.941248 8 0.0097598 1.85 4 A2_B1 A2_B2 bagpipes 3.162603 8 0.0028140 4.93 4 A2_B2 A2_B2 headless 3.040128 8 0.0093918 4.42 5 A1_B1 A2_B2 adequacy 2.158134 8 0.0020388 1.86 5 A1_B2 A2_B2 endeared 2.348948 8 0.0107752 1.78 5 A2_B1 A2_B2 typeface 2.297796 8 0.0023554 4.21 5 A2_B2 A2_B2 whiteout 2.172857 8 0.0092599 4.28 Here we can see that indeed, the different conditions are within the boundaries we set, and that the variables used as controls are matched for items with the same item_nr. 2.3 Plotting the Design While having data in long format is undoubtably useful, it’s not very efficient if you want to check what your stimuli look like in a quick glance (especially if your stimuli number in the thousands). What you probably want to do is plot the long format data to see how your stimuli differ between conditions and across matched items. Thankfully, LexOPS has a handy function to do exactly that: plot_design(stim) Figure 2.1: The results of the plot_design() function for the generated stimuli. Here, all the numeric variables used as independent variables or controls have their distributions plotted for each condition (in a grey violin plot). The points depict the values of individual words, and points of the same colour (joined by lines) are matched items. As we’d expect, our example stimuli show the expected differences in Bigram Probability and Concreteness, while Frequency is matched closely, and Length is matched exactly. 2.4 Generating as Many as Possible Let’s imagine that we’re not entirely sure how many stimuli we could generate using our design. It may be that the n = 25 we used earlier is considerably fewer than the number of stimuli we could generate with no problems. One way to test this is to have LexOPS generate as many stimuli as possible. We can do this by setting n = \"all\": possible_stim &lt;- LexOPS::lexops %&gt;% subset(PK.Brysbaert &gt;= 0.9) %&gt;% split_by(CNC.Brysbaert, 1:2 ~ 4:5) %&gt;% split_by(BG.SUBTLEX_UK, 0:0.003 ~ 0.009:0.013) %&gt;% control_for(Length, 0:0) %&gt;% control_for(Zipf.SUBTLEX_UK, -0.2:0.2) %&gt;% generate(n = &quot;all&quot;, match_null = &quot;random&quot;) This is much slower, as LexOPS will continue trying to generate combinations of words that fit the specified characteristics until it has exhausted all the possibilities. Nevertheless, we actually generated 102 words generated per condition. This number is likely to change slightly each time we run the pipeline, as different combinations are randomly made from all the possible combinations. That said, it is a fairly good indication of the number of possible stimuli we could generate. The 102 words we’ve managed to generate per condition here is quite a bit higher than the 25 we originally generated. Does this mean we should just request a larger stimulus list, such as n = 80? Well, it depends. If we want as many stimuli as are possible, then it may make sense to just set n = \"all\", but often we only want to use as many stimuli as we need to find our effect. Also, if we use as many combinations as possible, experimenters who want to replicate our effect using a different set of stimuli will likely have fewer novel combinations available to them. Note that when n = “all”, the default match_null = “balanced” is not recommended, as LexOPS will not know how many iterations will successfully generate items before the generate() function is actually run. LexOPS will give a warning if n = “all” and match_null = “balanced”, because the conditions selected as the match null will not actually be balanced in the generated stimuli. 2.5 Plotting Iterations It is also possible to check how well LexOPS performed when generating stimuli by plotting the cumulative item generation by iterations. To do this, we can use the plot_iterations() function. As an example, let’s see how well we generated the stimuli in the last section. plot_iterations(possible_stim) Figure 2.2: The cumulative number of items generated per iteration. Here, this shows a characteristic levelling-off; iterations become increasingly less likely to successfully generate items as the pool of possible combinations is gradually exhausted. "],
["matching-individual-words.html", "3 Matching Individual Words", " 3 Matching Individual Words While the generate pipeline is usually sufficient, it’s sometimes important to tailor stimuli more precisely. For instance, it may be important that a matched word is a plausible replacement for a target word in a sentence. The match_word() function exists for this purpose. This function is currently more cumbersome to use than the generate pipeline, but might be updated to use similar tidyverse-style syntax in the future. Here’s an example usage of match_word, to suggest a word matched for “elephant” in terms of: Length (exactly) Frequency (within ±0.25 Zipf) Imageability (within ±1, on a 1-7 Likert rating scale) Part of Speech (i.e. is also a noun) library(tidyverse) library(LexOPS) suggested_matches &lt;- lexops %&gt;% match_word( target = &quot;elephant&quot;, list( &quot;Length&quot;, c(&quot;Zipf.SUBTLEX_UK&quot;, -0.25, 0.25), c(&quot;IMAG.Glasgow_Norms&quot;, -1, 1), &quot;PoS.SUBTLEX_UK&quot; ) ) %&gt;% select(string, euclidean_distance, Length, Zipf.SUBTLEX_UK, IMAG.Glasgow_Norms, PoS.SUBTLEX_UK) The suggested matches are returned in a dataframe, filtered to be within the specified tolerances, and ordered by euclidean distance from the target word (calculated using all the numeric variables used). The closest suggested match for “elephant” is “sandwich”. If we are looking for a match to fit in a sentential context, we can choose the best suitable match from this list. string euclidean_distance Length Zipf.SUBTLEX_UK IMAG.Glasgow_Norms PoS.SUBTLEX_UK sandwich 0.1277845 8 4.246820 6.7647 noun trousers 0.2015595 8 4.244371 6.6286 noun wardrobe 0.3254995 8 4.104315 6.6176 noun clothing 0.3302426 8 4.135068 6.5455 noun calendar 0.3359636 8 4.329810 6.4000 noun magazine 0.3522378 8 4.287246 6.3846 noun bungalow 0.3726767 8 4.172277 6.4242 noun envelope 0.4004120 8 4.096792 6.4706 noun festival 0.4979158 8 4.510449 6.2353 noun motorway 0.5312793 8 4.107187 6.2333 noun exercise 0.5545788 8 4.449319 6.1212 noun treasure 0.5598275 8 4.458939 6.1176 noun portrait 0.5860690 8 4.183298 6.0968 noun engineer 0.6080041 8 4.138999 6.0909 noun document 0.6314306 8 4.182166 6.0323 noun shooting 0.7013990 8 4.391130 5.9032 noun applause 0.7068316 8 4.209885 5.9143 noun darkness 0.7291090 8 4.143049 5.9118 noun "]
]
